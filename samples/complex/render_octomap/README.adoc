== Rendering an OctMap
SLAM or Simultaneous Localization and Mapping is the main driving force behind modern robotics, AR. SLAM is a process which allows us to interact with the physical world.  The main step in Localization involves determining where the camera is at the present moment in relation to where it has been in the past.  The main idea is if you know where you were (a fix) and you know your momentum and direction, along with the time, you will be able to predict where you will be in the future.  This process is called Dead Reckoning.

When we want to do Dead Reckoning with a camera and an IMU, we're moving into  https://virtualrealitypop.com/a-tectonic-shift-in-augmented-reality-c095d0c69df[Visual Inertial Odometry].  However, this only gives us knowledge of where we are in relation to where we were, or Localization.  There is no map that is generated nor saved within that process.

So, at one frame we have a point cloud from our VIO efforts, and the next frame will have another point cloud. If we can register one point cloud with another, then we can join them into one point cloud.  This type of point cloud registration is commonly done with things like https://en.wikipedia.org/wiki/Iterative_closest_point[Iterative Closest Point].

Now, with our joined point cloud, we need to be able to recognize that sometimes, we've been places before, or we'd wind up with disjoint maps. That's where Loop Closure comes in.

The preceding high-level background describes how we get to a situation where there's plenty of desire to be able to work with and render point clouds that are dynamically generated.  ARCore, and ARKit are both able to create a point cloud map, and everything from Drones to robots use this same basic system to register and deal with the world around them.  To navigate a room, a drone/robot might need to be able to determine if a voxel is occupied or not by using an occupancy grid. This gives rise to solutions which are optimized for storing such maps that can be dynamically updated in real time.

The library https://octomap.github.io/[octomap] provides just such a library.

In this sample, we demonstrate how using instancing in Vulkan, we can dynamically display and update an Octomap that was generated by ARKit.  Using MoltenVK, we can display the map in real time, and communicate it rapidly over a network for desktops to render and combine in real time.
Here, we're simply showing how the render process works.  Additionally, we're also providing a Vulkan IMGui wrapper and demonstrating how it is used.  It's very similar if not nearly identical to the one found in the Framework, but it does have the ability to render images on the GUI and we use it to demonstrate how to place and size a viewport render context from a GUI to the rendering backend. This sample is meant to join several techniques in a way that mimics how a real world engine might work.

== Real-world Project background
https://www.holochip.com[Holochip] is an awardee on the U.S. Environmental Protection Agency (US EPA) Phase II SBIR program, Localization and Mapping AI Application (LAMA). LAMA is an iOS and Android compatible SLAM solution for disaster response teams. It will allow responders to map, localize, place markers, notes, voice recording, and use AI object detection in large spaces, and communicate that information with on-site coordinators in GPS- and network-denied environments.

[cols="a,a", frame=none, grid=none]
|===
| image::./images/mapping.png[mapping]
| image::./images/markers.png[markers]
|===

For further details on this project contact mailto:info@holochip.com[]
